{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import math\n",
    "# import numbers\n",
    "# import collections\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from PIL import Image, ImageOps\n",
    "# try:\n",
    "#     import accimage\n",
    "# except ImportError:\n",
    "#     accimage = None\n",
    "\n",
    "\n",
    "# class Compose(object):\n",
    "#     \"\"\"Composes several transforms together.\n",
    "#     Args:\n",
    "#         transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "#     Example:\n",
    "#         >>> transforms.Compose([\n",
    "#         >>>     transforms.CenterCrop(10),\n",
    "#         >>>     transforms.ToTensor(),\n",
    "#         >>> ])\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, transforms):\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         for t in self.transforms:\n",
    "#             img = t(img)\n",
    "#         return img\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         for t in self.transforms:\n",
    "#             t.randomize_parameters()\n",
    "\n",
    "\n",
    "# class ToTensor(object):\n",
    "#     \"\"\"Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n",
    "#     Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n",
    "#     [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, norm_value=255):\n",
    "#         self.norm_value = norm_value\n",
    "\n",
    "#     def __call__(self, pic):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
    "#         Returns:\n",
    "#             Tensor: Converted image.\n",
    "#         \"\"\"\n",
    "#         if isinstance(pic, np.ndarray):\n",
    "#             # handle numpy array\n",
    "#             img = torch.from_numpy(pic.transpose((2, 0, 1)))\n",
    "#             # backward compatibility\n",
    "#             return img.float().div(self.norm_value)\n",
    "\n",
    "#         if accimage is not None and isinstance(pic, accimage.Image):\n",
    "#             nppic = np.zeros(\n",
    "#                 [pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "#             pic.copyto(nppic)\n",
    "#             return torch.from_numpy(nppic)\n",
    "\n",
    "#         # handle PIL Image\n",
    "#         if pic.mode == 'I':\n",
    "#             img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
    "#         elif pic.mode == 'I;16':\n",
    "#             img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n",
    "#         else:\n",
    "#             img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "#         # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n",
    "#         if pic.mode == 'YCbCr':\n",
    "#             nchannel = 3\n",
    "#         elif pic.mode == 'I;16':\n",
    "#             nchannel = 1\n",
    "#         else:\n",
    "#             nchannel = len(pic.mode)\n",
    "#         img = img.view(pic.size[1], pic.size[0], nchannel)\n",
    "#         # put it from HWC to CHW format\n",
    "#         # yikes, this transpose takes 80% of the loading time/CPU\n",
    "#         img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "#         if isinstance(img, torch.ByteTensor):\n",
    "#             return img.float().div(self.norm_value)\n",
    "#         else:\n",
    "#             return img\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class Normalize(object):\n",
    "#     \"\"\"Normalize an tensor image with mean and standard deviation.\n",
    "#     Given mean: (R, G, B) and std: (R, G, B),\n",
    "#     will normalize each channel of the torch.*Tensor, i.e.\n",
    "#     channel = (channel - mean) / std\n",
    "#     Args:\n",
    "#         mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "#         std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "#             respecitvely.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, mean, std):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "\n",
    "#     def __call__(self, tensor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "#         Returns:\n",
    "#             Tensor: Normalized image.\n",
    "#         \"\"\"\n",
    "#         # TODO: make efficient\n",
    "#         for t, m, s in zip(tensor, self.mean, self.std):\n",
    "#             t.sub_(m).div_(s)\n",
    "#         return tensor\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class Scale(object):\n",
    "#     \"\"\"Rescale the input PIL.Image to the given size.\n",
    "#     Args:\n",
    "#         size (sequence or int): Desired output size. If size is a sequence like\n",
    "#             (w, h), output size will be matched to this. If size is an int,\n",
    "#             smaller edge of the image will be matched to this number.\n",
    "#             i.e, if height > width, then image will be rescaled to\n",
    "#             (size * height / width, size)\n",
    "#         interpolation (int, optional): Desired interpolation. Default is\n",
    "#             ``PIL.Image.BILINEAR``\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "#         assert isinstance(size,\n",
    "#                           int) or (isinstance(size, collections.Iterable) and\n",
    "#                                    len(size) == 2)\n",
    "#         self.size = size\n",
    "#         self.interpolation = interpolation\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             img (PIL.Image): Image to be scaled.\n",
    "#         Returns:\n",
    "#             PIL.Image: Rescaled image.\n",
    "#         \"\"\"\n",
    "#         if isinstance(self.size, int):\n",
    "#             w, h = img.size\n",
    "#             if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "#                 return img\n",
    "#             if w < h:\n",
    "#                 ow = self.size\n",
    "#                 oh = int(self.size * h / w)\n",
    "#                 return img.resize((ow, oh), self.interpolation)\n",
    "#             else:\n",
    "#                 oh = self.size\n",
    "#                 ow = int(self.size * w / h)\n",
    "#                 return img.resize((ow, oh), self.interpolation)\n",
    "#         else:\n",
    "#             return img.resize(self.size, self.interpolation)\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class CenterCrop(object):\n",
    "#     \"\"\"Crops the given PIL.Image at the center.\n",
    "#     Args:\n",
    "#         size (sequence or int): Desired output size of the crop. If size is an\n",
    "#             int instead of sequence like (h, w), a square crop (size, size) is\n",
    "#             made.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, size):\n",
    "#         if isinstance(size, numbers.Number):\n",
    "#             self.size = (int(size), int(size))\n",
    "#         else:\n",
    "#             self.size = size\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             img (PIL.Image): Image to be cropped.\n",
    "#         Returns:\n",
    "#             PIL.Image: Cropped image.\n",
    "#         \"\"\"\n",
    "#         w, h = img.size\n",
    "#         th, tw = self.size\n",
    "#         x1 = int(round((w - tw) / 2.))\n",
    "#         y1 = int(round((h - th) / 2.))\n",
    "#         return img.crop((x1, y1, x1 + tw, y1 + th))\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class CornerCrop(object):\n",
    "\n",
    "#     def __init__(self, size, crop_position=None):\n",
    "#         self.size = size\n",
    "#         if crop_position is None:\n",
    "#             self.randomize = True\n",
    "#         else:\n",
    "#             self.randomize = False\n",
    "#         self.crop_position = crop_position\n",
    "#         self.crop_positions = ['c', 'tl', 'tr', 'bl', 'br']\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         image_width = img.size[0]\n",
    "#         image_height = img.size[1]\n",
    "\n",
    "#         if self.crop_position == 'c':\n",
    "#             th, tw = (self.size, self.size)\n",
    "#             x1 = int(round((image_width - tw) / 2.))\n",
    "#             y1 = int(round((image_height - th) / 2.))\n",
    "#             x2 = x1 + tw\n",
    "#             y2 = y1 + th\n",
    "#         elif self.crop_position == 'tl':\n",
    "#             x1 = 0\n",
    "#             y1 = 0\n",
    "#             x2 = self.size\n",
    "#             y2 = self.size\n",
    "#         elif self.crop_position == 'tr':\n",
    "#             x1 = image_width - self.size\n",
    "#             y1 = 0\n",
    "#             x2 = image_width\n",
    "#             y2 = self.size\n",
    "#         elif self.crop_position == 'bl':\n",
    "#             x1 = 0\n",
    "#             y1 = image_height - self.size\n",
    "#             x2 = self.size\n",
    "#             y2 = image_height\n",
    "#         elif self.crop_position == 'br':\n",
    "#             x1 = image_width - self.size\n",
    "#             y1 = image_height - self.size\n",
    "#             x2 = image_width\n",
    "#             y2 = image_height\n",
    "\n",
    "#         img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "#         return img\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         if self.randomize:\n",
    "#             self.crop_position = self.crop_positions[random.randint(\n",
    "#                 0,\n",
    "#                 len(self.crop_positions) - 1)]\n",
    "\n",
    "\n",
    "# class RandomHorizontalFlip(object):\n",
    "#     \"\"\"Horizontally flip the given PIL.Image randomly with a probability of 0.5.\"\"\"\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             img (PIL.Image): Image to be flipped.\n",
    "#         Returns:\n",
    "#             PIL.Image: Randomly flipped image.\n",
    "#         \"\"\"\n",
    "#         if self.p < 0.5:\n",
    "#             return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#         return img\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         self.p = random.random()\n",
    "\n",
    "\n",
    "# class MultiScaleCornerCrop(object):\n",
    "#     \"\"\"Crop the given PIL.Image to randomly selected size.\n",
    "#     A crop of size is selected from scales of the original size.\n",
    "#     A position of cropping is randomly selected from 4 corners and 1 center.\n",
    "#     This crop is finally resized to given size.\n",
    "#     Args:\n",
    "#         scales: cropping scales of the original size\n",
    "#         size: size of the smaller edge\n",
    "#         interpolation: Default: PIL.Image.BILINEAR\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  scales,\n",
    "#                  size,\n",
    "#                  interpolation=Image.BILINEAR,\n",
    "#                  crop_positions=['c', 'tl', 'tr', 'bl', 'br']):\n",
    "#         self.scales = scales\n",
    "#         self.size = size\n",
    "#         self.interpolation = interpolation\n",
    "\n",
    "#         self.crop_positions = crop_positions\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         min_length = min(img.size[0], img.size[1])\n",
    "#         crop_size = int(min_length * self.scale)\n",
    "\n",
    "#         image_width = img.size[0]\n",
    "#         image_height = img.size[1]\n",
    "\n",
    "#         if self.crop_position == 'c':\n",
    "#             center_x = image_width // 2\n",
    "#             center_y = image_height // 2\n",
    "#             box_half = crop_size // 2\n",
    "#             x1 = center_x - box_half\n",
    "#             y1 = center_y - box_half\n",
    "#             x2 = center_x + box_half\n",
    "#             y2 = center_y + box_half\n",
    "#         elif self.crop_position == 'tl':\n",
    "#             x1 = 0\n",
    "#             y1 = 0\n",
    "#             x2 = crop_size\n",
    "#             y2 = crop_size\n",
    "#         elif self.crop_position == 'tr':\n",
    "#             x1 = image_width - crop_size\n",
    "#             y1 = 0\n",
    "#             x2 = image_width\n",
    "#             y2 = crop_size\n",
    "#         elif self.crop_position == 'bl':\n",
    "#             x1 = 0\n",
    "#             y1 = image_height - crop_size\n",
    "#             x2 = crop_size\n",
    "#             y2 = image_height\n",
    "#         elif self.crop_position == 'br':\n",
    "#             x1 = image_width - crop_size\n",
    "#             y1 = image_height - crop_size\n",
    "#             x2 = image_width\n",
    "#             y2 = image_height\n",
    "\n",
    "#         img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "#         return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "#         self.crop_position = self.crop_positions[random.randint(\n",
    "#             0,\n",
    "#             len(self.scales) - 1)]\n",
    "\n",
    "\n",
    "# class MultiScaleRandomCrop(object):\n",
    "\n",
    "#     def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
    "#         self.scales = scales\n",
    "#         self.size = size\n",
    "#         self.interpolation = interpolation\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         min_length = min(img.size[0], img.size[1])\n",
    "#         crop_size = int(min_length * self.scale)\n",
    "\n",
    "#         image_width = img.size[0]\n",
    "#         image_height = img.size[1]\n",
    "\n",
    "#         x1 = self.tl_x * (image_width - crop_size)\n",
    "#         y1 = self.tl_y * (image_height - crop_size)\n",
    "#         x2 = x1 + crop_size\n",
    "#         y2 = y1 + crop_size\n",
    "\n",
    "#         img = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "#         return img.resize((self.size, self.size), self.interpolation)\n",
    "\n",
    "#     def randomize_parameters(self):\n",
    "#         self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "#         self.tl_x = random.random()\n",
    "#         self.tl_y = random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import math\n",
    "\n",
    "\n",
    "# class Compose(object):\n",
    "\n",
    "#     def __init__(self, transforms):\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __call__(self, target):\n",
    "#         dst = []\n",
    "#         for t in self.transforms:\n",
    "#             dst.append(t(target))\n",
    "#         return dst\n",
    "\n",
    "\n",
    "# class ClassLabel(object):\n",
    "\n",
    "#     def __call__(self, target):\n",
    "#         return target['label']\n",
    "\n",
    "\n",
    "# class VideoID(object):\n",
    "\n",
    "#     def __call__(self, target):\n",
    "#         return target['video_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import math\n",
    "\n",
    "\n",
    "# class LoopPadding(object):\n",
    "\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "\n",
    "#     def __call__(self, frame_indices):\n",
    "#         out = frame_indices\n",
    "\n",
    "#         for index in out:\n",
    "#             if len(out) >= self.size:\n",
    "#                 break\n",
    "#             out.append(index)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class TemporalBeginCrop(object):\n",
    "#     \"\"\"Temporally crop the given frame indices at a beginning.\n",
    "#     If the number of frames is less than the size,\n",
    "#     loop the indices as many times as necessary to satisfy the size.\n",
    "#     Args:\n",
    "#         size (int): Desired output size of the crop.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "\n",
    "#     def __call__(self, frame_indices):\n",
    "#         out = frame_indices[:self.size]\n",
    "\n",
    "#         for index in out:\n",
    "#             if len(out) >= self.size:\n",
    "#                 break\n",
    "#             out.append(index)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class TemporalCenterCrop(object):\n",
    "#     \"\"\"Temporally crop the given frame indices at a center.\n",
    "#     If the number of frames is less than the size,\n",
    "#     loop the indices as many times as necessary to satisfy the size.\n",
    "#     Args:\n",
    "#         size (int): Desired output size of the crop.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "\n",
    "#     def __call__(self, frame_indices):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             frame_indices (list): frame indices to be cropped.\n",
    "#         Returns:\n",
    "#             list: Cropped frame indices.\n",
    "#         \"\"\"\n",
    "\n",
    "#         center_index = len(frame_indices) // 2\n",
    "#         begin_index = max(0, center_index - (self.size // 2))\n",
    "#         end_index = min(begin_index + self.size, len(frame_indices))\n",
    "\n",
    "#         out = frame_indices[begin_index:end_index]\n",
    "\n",
    "#         for index in out:\n",
    "#             if len(out) >= self.size:\n",
    "#                 break\n",
    "#             out.append(index)\n",
    "\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class TemporalRandomCrop(object):\n",
    "#     \"\"\"Temporally crop the given frame indices at a random location.\n",
    "#     If the number of frames is less than the size,\n",
    "#     loop the indices as many times as necessary to satisfy the size.\n",
    "#     Args:\n",
    "#         size (int): Desired output size of the crop.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size\n",
    "\n",
    "#     def __call__(self, frame_indices):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             frame_indices (list): frame indices to be cropped.\n",
    "#         Returns:\n",
    "#             list: Cropped frame indices.\n",
    "#         \"\"\"\n",
    "\n",
    "#         rand_end = max(0, len(frame_indices) - self.size - 1)\n",
    "#         begin_index = random.randint(0, rand_end)\n",
    "#         end_index = min(begin_index + self.size, len(frame_indices))\n",
    "\n",
    "#         out = frame_indices[begin_index:end_index]\n",
    "\n",
    "#         for index in out:\n",
    "#             if len(out) >= self.size:\n",
    "#                 break\n",
    "#             out.append(index)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import functools\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from utils import load_value_file\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    try:\n",
    "        import accimage\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def get_default_image_loader():\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader\n",
    "    else:\n",
    "        return pil_loader\n",
    "\n",
    "\n",
    "def video_loader(video_dir_path, frame_indices, image_loader):\n",
    "    video = []\n",
    "    for i in frame_indices:\n",
    "        image_path = os.path.join(video_dir_path, 'image_{:05d}.jpg'.format(i))\n",
    "        if os.path.exists(image_path):\n",
    "            video.append(image_loader(image_path))\n",
    "        else:\n",
    "            return video\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "def get_default_video_loader():\n",
    "    image_loader = get_default_image_loader()\n",
    "    return functools.partial(video_loader, image_loader=image_loader)\n",
    "\n",
    "\n",
    "def load_annotation_data(data_file_path):\n",
    "    with open(data_file_path, 'r') as data_file:\n",
    "        return json.load(data_file)\n",
    "\n",
    "\n",
    "def get_class_labels(data):\n",
    "    class_names = []\n",
    "    index = 0\n",
    "    for node1 in data['taxonomy']:\n",
    "        is_leaf = True\n",
    "        for node2 in data['taxonomy']:\n",
    "            if node2['parentId'] == node1['nodeId']:\n",
    "                is_leaf = False\n",
    "                break\n",
    "        if is_leaf:\n",
    "            class_names.append(node1['nodeName'])\n",
    "\n",
    "    class_labels_map = {}\n",
    "\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_labels_map[class_name] = i\n",
    "\n",
    "    return class_labels_map\n",
    "\n",
    "\n",
    "def get_video_names_and_annotations(data, subset):\n",
    "    video_names = []\n",
    "    annotations = []\n",
    "\n",
    "    for key, value in data['database'].items():\n",
    "        this_subset = value['subset']\n",
    "        if this_subset == subset:\n",
    "            if subset == 'testing':\n",
    "                video_names.append('v_{}'.format(key))\n",
    "            else:\n",
    "                video_names.append('v_{}'.format(key))\n",
    "                annotations.append(value['annotations'])\n",
    "\n",
    "    return video_names, annotations\n",
    "\n",
    "\n",
    "def modify_frame_indices(video_dir_path, frame_indices):\n",
    "    modified_indices = []\n",
    "    for i in frame_indices:\n",
    "        image_path = os.path.join(video_dir_path, 'image_{:05d}.jpg'.format(i))\n",
    "        if not os.path.exists(image_path):\n",
    "            return modified_indices\n",
    "        modified_indices.append(i)\n",
    "    return modified_indices\n",
    "\n",
    "\n",
    "#m_captions\n",
    "def make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n",
    "                 sample_duration):\n",
    "    \n",
    "    caption_path = 'data/captions/train.json'\n",
    "    captions_json = json.load(open(caption_path, 'r'))\n",
    "    \n",
    "    data = load_annotation_data(annotation_path)\n",
    "    video_names, annotations = get_video_names_and_annotations(data, subset)\n",
    "    \n",
    "    #class_to_idx = get_class_labels(data)\n",
    "    idx_to_class = {}\n",
    "    #for name, label in class_to_idx.items():\n",
    "        #idx_to_class[label] = name\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(len(video_names)):\n",
    "        if i % 1000 == 0:\n",
    "            print('dataset loading [{}/{}]'.format(i, len(video_names)))\n",
    "\n",
    "        video_path = os.path.join(root_path, video_names[i])\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        fps_file_path = os.path.join(video_path, 'fps')\n",
    "        fps = load_value_file(fps_file_path)\n",
    "        video_id = video_names[i]\n",
    "        if video_id not in captions_json:\n",
    "            print(\"Warning: caption not found for \",video_id)\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Caption found for \",video_id)\n",
    "\n",
    "        for j in range(len(captions_json[video_id]['timestamps'])):\n",
    "            begin_t = math.ceil(captions_json[video_id]['timestamps'][j][0] * fps)\n",
    "            end_t = math.ceil(captions_json[video_id]['timestamps'][j][1] * fps)\n",
    "            if begin_t == 0:\n",
    "                begin_t = 1\n",
    "            n_frames = end_t - begin_t\n",
    "\n",
    "            sample = {\n",
    "                'video': video_path,\n",
    "                'segment': [begin_t, end_t],\n",
    "                'fps': fps,\n",
    "                'video_id': video_names[i][2:],\n",
    "            }\n",
    "            if(j < len(captions_json[video_id]['timestamps'])):\n",
    "                sample['caption'] = captions_json[video_id]['sentences'][j]\n",
    "            else:\n",
    "                sample['caption'] = -1\n",
    "\n",
    "\n",
    "#             if len(annotations) != 0:\n",
    "#                 sample['label'] = class_to_idx[annotation['label']]\n",
    "#             else:\n",
    "#                 sample['label'] = -1\n",
    "\n",
    "            if n_samples_for_each_video == 1:\n",
    "                frame_indices = list(range(begin_t, end_t))\n",
    "                frame_indices = modify_frame_indices(sample['video'],\n",
    "                                                     frame_indices)\n",
    "                if len(frame_indices) < 16:\n",
    "                    continue\n",
    "                sample['frame_indices'] = frame_indices\n",
    "                dataset.append(sample)\n",
    "            else:\n",
    "                if n_samples_for_each_video > 1:\n",
    "                    step = max(1,\n",
    "                               math.ceil((n_frames - 1 - sample_duration) /\n",
    "                                         (n_samples_for_each_video - 1)))\n",
    "                else:\n",
    "                    step = sample_duration\n",
    "                for j in range(begin_t, end_t, step):\n",
    "                    sample_j = copy.deepcopy(sample)\n",
    "                    frame_indices = list(range(j, j + sample_duration))\n",
    "                    frame_indices = modify_frame_indices(\n",
    "                        sample_j['video'], frame_indices)\n",
    "                    if len(frame_indices) < 16:\n",
    "                        continue\n",
    "                    sample_j['frame_indices'] = frame_indices\n",
    "                    dataset.append(sample_j)\n",
    "\n",
    "    return dataset, idx_to_class\n",
    "\n",
    "#m_dataset\n",
    "# def make_dataset(root_path, annotation_path, subset, n_samples_for_each_video,\n",
    "#                  sample_duration):\n",
    "#     data = load_annotation_data(annotation_path)\n",
    "#     video_names, annotations = get_video_names_and_annotations(data, subset)\n",
    "#     class_to_idx = get_class_labels(data)\n",
    "#     idx_to_class = {}\n",
    "#     for name, label in class_to_idx.items():\n",
    "#         idx_to_class[label] = name\n",
    "\n",
    "#     dataset = []\n",
    "#     for i in range(len(video_names)):\n",
    "#         if i % 1000 == 0:\n",
    "#             print('dataset loading [{}/{}]'.format(i, len(video_names)))\n",
    "\n",
    "#         video_path = os.path.join(root_path, video_names[i])\n",
    "#         if not os.path.exists(video_path):\n",
    "#             continue\n",
    "\n",
    "#         fps_file_path = os.path.join(video_path, 'fps')\n",
    "#         fps = load_value_file(fps_file_path)\n",
    "\n",
    "#         for annotation in annotations[i]:\n",
    "#             begin_t = math.ceil(annotation['segment'][0] * fps)\n",
    "#             end_t = math.ceil(annotation['segment'][1] * fps)\n",
    "#             if begin_t == 0:\n",
    "#                 begin_t = 1\n",
    "#             n_frames = end_t - begin_t\n",
    "\n",
    "#             sample = {\n",
    "#                 'video': video_path,\n",
    "#                 'segment': [begin_t, end_t],\n",
    "#                 'fps': fps,\n",
    "#                 'video_id': video_names[i][2:]\n",
    "#             }\n",
    "#             if len(annotations) != 0:\n",
    "#                 sample['label'] = class_to_idx[annotation['label']]\n",
    "#             else:\n",
    "#                 sample['label'] = -1\n",
    "\n",
    "#             if n_samples_for_each_video == 1:\n",
    "#                 frame_indices = list(range(begin_t, end_t))\n",
    "#                 frame_indices = modify_frame_indices(sample['video'],\n",
    "#                                                      frame_indices)\n",
    "#                 if len(frame_indices) < 16:\n",
    "#                     continue\n",
    "#                 sample['frame_indices'] = frame_indices\n",
    "#                 dataset.append(sample)\n",
    "#             else:\n",
    "#                 if n_samples_for_each_video > 1:\n",
    "#                     step = max(1,\n",
    "#                                math.ceil((n_frames - 1 - sample_duration) /\n",
    "#                                          (n_samples_for_each_video - 1)))\n",
    "#                 else:\n",
    "#                     step = sample_duration\n",
    "#                 for j in range(begin_t, end_t, step):\n",
    "#                     sample_j = copy.deepcopy(sample)\n",
    "#                     frame_indices = list(range(j, j + sample_duration))\n",
    "#                     frame_indices = modify_frame_indices(\n",
    "#                         sample_j['video'], frame_indices)\n",
    "#                     if len(frame_indices) < 16:\n",
    "#                         continue\n",
    "#                     sample_j['frame_indices'] = frame_indices\n",
    "#                     dataset.append(sample_j)\n",
    "\n",
    "#     return dataset, idx_to_class\n",
    "\n",
    "\n",
    "def get_end_t(video_path):\n",
    "    file_names = os.listdir(video_path)\n",
    "    image_file_names = [x for x in file_names if 'image' in x]\n",
    "    image_file_names.sort(reverse=True)\n",
    "    return int(image_file_names[0][6:11])\n",
    "\n",
    "\n",
    "def make_untrimmed_dataset(root_path, annotation_path, subset,\n",
    "                           n_samples_for_each_video, sample_duration):\n",
    "    data = load_annotation_data(annotation_path)\n",
    "    video_names, _ = get_video_names_and_annotations(data, subset)\n",
    "    class_to_idx = get_class_labels(data)\n",
    "    captions_path = 'data/captions/train.json'\n",
    "    captions_json = json.load(open(captions_path, 'r'))\n",
    "    \n",
    "    idx_to_class = {}\n",
    "    for name, label in class_to_idx.items():\n",
    "        idx_to_class[label] = name\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(len(video_names)):\n",
    "        if i % 1000 == 0:\n",
    "            print('dataset loading [{}/{}]'.format(i, len(video_names)))\n",
    "\n",
    "        video_path = os.path.join(root_path, video_names[i])\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        fps_file_path = os.path.join(video_path, 'fps')\n",
    "        fps = load_value_file(fps_file_path)\n",
    "\n",
    "        begin_t = 1\n",
    "        end_t = get_end_t(video_path)\n",
    "        n_frames = end_t - begin_t\n",
    "        \n",
    "        sample = {\n",
    "            'video': video_path,\n",
    "            'segment': [begin_t, end_t],\n",
    "            'fps': fps,\n",
    "            'video_id': video_names[i][2:]\n",
    "        }\n",
    "\n",
    "        if n_samples_for_each_video >= 1:\n",
    "            step = max(1,\n",
    "                       math.ceil((n_frames - 1 - sample_duration) /\n",
    "                                 (n_samples_for_each_video - 1)))\n",
    "        else:\n",
    "            step = sample_duration\n",
    "        for j in range(begin_t, end_t, step):\n",
    "            sample_j = copy.deepcopy(sample)\n",
    "            frame_indices = list(range(j, j + sample_duration))\n",
    "            frame_indices = modify_frame_indices(sample_j['video'],\n",
    "                                                 frame_indices)\n",
    "            if len(frame_indices) < 16:\n",
    "                continue\n",
    "            sample_j['frame_indices'] = frame_indices\n",
    "            dataset.append(sample_j)\n",
    "\n",
    "    return dataset, idx_to_class\n",
    "\n",
    "\n",
    "class ActivityNet(data.Dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root (string): Root directory path.\n",
    "        spatial_transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        temporal_transform (callable, optional): A function/transform that  takes in a list of frame indices\n",
    "            and returns a transformed version\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        loader (callable, optional): A function to load an video given its path and frame indices.\n",
    "     Attributes:\n",
    "        classes (list): List of the class names.\n",
    "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
    "        imgs (list): List of (image path, class_index) tuples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 annotation_path,\n",
    "                 subset,\n",
    "                 is_untrimmed_setting=False,\n",
    "                 n_samples_for_each_video=1,\n",
    "                 spatial_transform=None,\n",
    "                 temporal_transform=None,\n",
    "                 target_transform=None,\n",
    "                 sample_duration=16,\n",
    "                 get_loader=get_default_video_loader):\n",
    "        if is_untrimmed_setting:\n",
    "            self.data, self.class_names = make_untrimmed_dataset(\n",
    "                root_path, annotation_path, subset, n_samples_for_each_video,\n",
    "                sample_duration)\n",
    "        else:\n",
    "            self.data, self.class_names = make_dataset(\n",
    "                root_path, annotation_path, subset, n_samples_for_each_video,\n",
    "                sample_duration)\n",
    "\n",
    "        self.spatial_transform = spatial_transform\n",
    "        self.temporal_transform = temporal_transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = get_loader()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path = self.data[index]['video']\n",
    "\n",
    "        frame_indices = self.data[index]['frame_indices']\n",
    "        if self.temporal_transform is not None:\n",
    "            frame_indices = self.temporal_transform(frame_indices)\n",
    "        clip = self.loader(path, frame_indices)\n",
    "        if self.spatial_transform is not None:\n",
    "            self.spatial_transform.randomize_parameters()\n",
    "            clip = [self.spatial_transform(img) for img in clip]\n",
    "        clip = torch.stack(clip, 0).permute(1, 0, 2, 3)\n",
    "\n",
    "        target = self.data[index]\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return clip, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m_opts\n",
    "import argparse\n",
    "\n",
    "\n",
    "def parse_opts():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    print(type(parser))\n",
    "    parser.add_argument(\n",
    "        '--root_path',\n",
    "        #default='/root/data/ActivityNet',\n",
    "        default='data',\n",
    "        type=str,\n",
    "        help='Root directory path of data')\n",
    "    parser.add_argument(\n",
    "        '--video_path',\n",
    "        #default='video_kinetics_jpg',\n",
    "        default='jpg_videos',\n",
    "        type=str,\n",
    "        help='Directory path of Videos')\n",
    "    parser.add_argument(\n",
    "        '--annotation_path',\n",
    "        #default='kinetics.json',\n",
    "        default='activitynet.json',\n",
    "        type=str,\n",
    "        help='Annotation file path')\n",
    "    parser.add_argument(\n",
    "        '--result_path',\n",
    "        default='results',\n",
    "        type=str,\n",
    "        help='Result directory path')\n",
    "    parser.add_argument(\n",
    "        '--dataset',\n",
    "        default='activitynet',\n",
    "        #default='kinetics',\n",
    "        type=str,\n",
    "        help='Used dataset (activitynet | kinetics | ucf101 | hmdb51)')\n",
    "    parser.add_argument(\n",
    "        '--n_classes',\n",
    "        default=200,\n",
    "        type=int,\n",
    "        help=\n",
    "        'Number of classes (activitynet: 200, kinetics: 400, ucf101: 101, hmdb51: 51)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_finetune_classes',\n",
    "        default=400,\n",
    "        type=int,\n",
    "        help=\n",
    "        'Number of classes for fine-tuning. n_classes is set to the number when pretraining.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--sample_size',\n",
    "        default=112,\n",
    "        type=int,\n",
    "        help='Height and width of inputs')\n",
    "    parser.add_argument(\n",
    "        '--sample_duration',\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help='Temporal duration of inputs')\n",
    "    parser.add_argument(\n",
    "        '--initial_scale',\n",
    "        default=1.0,\n",
    "        type=float,\n",
    "        help='Initial scale for multiscale cropping')\n",
    "    parser.add_argument(\n",
    "        '--n_scales',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='Number of scales for multiscale cropping')\n",
    "    parser.add_argument(\n",
    "        '--scale_step',\n",
    "        default=0.84089641525,\n",
    "        type=float,\n",
    "        help='Scale step for multiscale cropping')\n",
    "    parser.add_argument(\n",
    "        '--train_crop',\n",
    "        default='corner',\n",
    "        type=str,\n",
    "        help=\n",
    "        'Spatial cropping method in training. random is uniform. corner is selection from 4 corners and 1 center.  (random | corner | center)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help=\n",
    "        'Initial learning rate (divided by 10 while training by lr scheduler)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, help='Momentum')\n",
    "    parser.add_argument(\n",
    "        '--dampening', default=0.9, type=float, help='dampening of SGD')\n",
    "    parser.add_argument(\n",
    "        '--weight_decay', default=1e-3, type=float, help='Weight Decay')\n",
    "    parser.add_argument(\n",
    "        '--mean_dataset',\n",
    "        default='activitynet',\n",
    "        type=str,\n",
    "        help=\n",
    "        'dataset for mean values of mean subtraction (activitynet | kinetics)')\n",
    "    parser.add_argument(\n",
    "        '--no_mean_norm',\n",
    "        action='store_true',\n",
    "        help='If true, inputs are not normalized by mean.')\n",
    "    parser.set_defaults(no_mean_norm=False)\n",
    "    parser.add_argument(\n",
    "        '--std_norm',\n",
    "        action='store_true',\n",
    "        help='If true, inputs are normalized by standard deviation.')\n",
    "    parser.set_defaults(std_norm=False)\n",
    "    parser.add_argument(\n",
    "        '--nesterov', action='store_true', help='Nesterov momentum')\n",
    "    parser.set_defaults(nesterov=False)\n",
    "    parser.add_argument(\n",
    "        '--optimizer',\n",
    "        default='sgd',\n",
    "        type=str,\n",
    "        help='Currently only support SGD')\n",
    "    parser.add_argument(\n",
    "        '--lr_patience',\n",
    "        default=10,\n",
    "        type=int,\n",
    "        help='Patience of LR scheduler. See documentation of ReduceLROnPlateau.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size', default=1, type=int, help='Batch Size')\n",
    "        #'--batch_size', default=128, type=int, help='Batch Size')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--n_epochs',\n",
    "        default=3,\n",
    "        type=int,\n",
    "        help='Number of total epochs to run')\n",
    "    parser.add_argument(\n",
    "        '--begin_epoch',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\n",
    "        'Training begins at this epoch. Previous trained model indicated by resume_path is loaded.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_val_samples',\n",
    "        default=3,\n",
    "        type=int,\n",
    "        help='Number of validation samples for each activity')\n",
    "    parser.add_argument(\n",
    "        '--resume_path',\n",
    "        default='',\n",
    "        type=str,\n",
    "        help='Save data (.pth) of previous training')\n",
    "    parser.add_argument(\n",
    "        '--pretrain_path', default='', type=str, help='Pretrained model (.pth)')\n",
    "    parser.add_argument(\n",
    "        '--ft_begin_index',\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help='Begin block index of fine-tuning')\n",
    "    parser.add_argument(\n",
    "        '--no_train',\n",
    "        action='store_true',\n",
    "        help='If true, training is not performed.')\n",
    "    parser.set_defaults(no_train=False)\n",
    "    parser.add_argument(\n",
    "        '--no_val',\n",
    "        action='store_true',\n",
    "        help='If true, validation is not performed.')\n",
    "    parser.set_defaults(no_val=True)\n",
    "    parser.add_argument(\n",
    "        '--test', action='store_true', help='If true, test is performed.')\n",
    "    parser.set_defaults(test=False)\n",
    "    parser.add_argument(\n",
    "        '--test_subset',\n",
    "        default='val',\n",
    "        type=str,\n",
    "        help='Used subset in test (val | test)')\n",
    "    parser.add_argument(\n",
    "        '--scale_in_test',\n",
    "        default=1.0,\n",
    "        type=float,\n",
    "        help='Spatial scale in test')\n",
    "    parser.add_argument(\n",
    "        '--crop_position_in_test',\n",
    "        default='c',\n",
    "        type=str,\n",
    "        help='Cropping method (c | tl | tr | bl | br) in test')\n",
    "    parser.add_argument(\n",
    "        '--no_softmax_in_test',\n",
    "        action='store_true',\n",
    "        help='If true, output for each clip is not normalized using softmax.')\n",
    "    parser.set_defaults(no_softmax_in_test=False)\n",
    "    parser.add_argument(\n",
    "        '--no_cuda', action='store_true', help='If true, cuda is not used.')\n",
    "    parser.set_defaults(no_cuda=True)\n",
    "    parser.add_argument(\n",
    "        '--n_threads',\n",
    "        default=4,\n",
    "        type=int,\n",
    "        help='Number of threads for multi-thread loading')\n",
    "    parser.add_argument(\n",
    "        '--checkpoint',\n",
    "        default=10,\n",
    "        type=int,\n",
    "        help='Trained model is saved at every this epochs.')\n",
    "    parser.add_argument(\n",
    "        '--no_hflip',\n",
    "        action='store_true',\n",
    "        help='If true holizontal flipping is not performed.')\n",
    "    parser.set_defaults(no_hflip=False)\n",
    "    parser.add_argument(\n",
    "        '--norm_value',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\n",
    "        'If 1, range of inputs is [0-255]. If 255, range of inputs is [0-1].')\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        default='resnet',\n",
    "        type=str,\n",
    "        help='(resnet | preresnet | wideresnet | resnext | densenet | ')\n",
    "    parser.add_argument(\n",
    "        '--model_depth',\n",
    "        default=10,\n",
    "        type=int,\n",
    "        help='Depth of resnet (10 | 18 | 34 | 50 | 101)')\n",
    "    parser.add_argument(\n",
    "        '--resnet_shortcut',\n",
    "        default='B',\n",
    "        type=str,\n",
    "        help='Shortcut type of resnet (A | B)')\n",
    "    parser.add_argument(\n",
    "        '--wide_resnet_k', default=2, type=int, help='Wide resnet k')\n",
    "    parser.add_argument(\n",
    "        '--resnext_cardinality',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='ResNeXt cardinality')\n",
    "    parser.add_argument(\n",
    "        '--manual_seed', default=1, type=int, help='Manually set random seed')\n",
    "    print(\"almost done\")\n",
    "    import sys\n",
    "    sys.argv=['']\n",
    "    del sys\n",
    "    args = parser.parse_args()\n",
    "    print(\"done\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "__all__ = [\n",
    "    'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "    'resnet152', 'resnet200'\n",
    "]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    # 3x3x3 convolution with padding\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)\n",
    "\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(\n",
    "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
    "        out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "\n",
    "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 sample_size,\n",
    "                 sample_duration,\n",
    "                 shortcut_type='B',\n",
    "                 num_classes=400):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            3,\n",
    "            64,\n",
    "            kernel_size=7,\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(3, 3, 3),\n",
    "            bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], shortcut_type, stride=2)\n",
    "        last_duration = int(math.ceil(sample_duration / 16))\n",
    "        last_size = int(math.ceil(sample_size / 32))\n",
    "        self.avgpool = nn.AvgPool3d(\n",
    "            (last_duration, last_size, last_size), stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    "\n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "    ft_module_names.append('fc')\n",
    "\n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'params': v, 'lr': 0.0})\n",
    "\n",
    "    return parameters\n",
    "\n",
    "#m_models\n",
    "\n",
    "def resnet10(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet200(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'models/')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#import resnet, pre_act_resnet, wide_resnet, resnext, densenet\n",
    "#m_generate\n",
    "\n",
    "def generate_model(opt):\n",
    "    assert opt.model in [\n",
    "        'resnet', 'preresnet', 'wideresnet', 'resnext', 'densenet'\n",
    "    ]\n",
    "\n",
    "    if opt.model == 'resnet':\n",
    "        assert opt.model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "        #from resnet import get_fine_tuning_parameters\n",
    "\n",
    "        if opt.model_depth == 10:\n",
    "            model = resnet10(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 18:\n",
    "            model = resnet18(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 34:\n",
    "            model = resnet34(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 50:\n",
    "            model = resnet50(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 101:\n",
    "            model = resnet101(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 152:\n",
    "            model = resnet152(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 200:\n",
    "            model = resnet200(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "    elif opt.model == 'wideresnet':\n",
    "        assert opt.model_depth in [50]\n",
    "\n",
    "        #from models.wide_resnet import get_fine_tuning_parameters\n",
    "\n",
    "        if opt.model_depth == 50:\n",
    "            model = resnet50(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                k=opt.wide_resnet_k,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "    elif opt.model == 'resnext':\n",
    "        assert opt.model_depth in [50, 101, 152]\n",
    "\n",
    "        #from models.resnext import get_fine_tuning_parameters\n",
    "\n",
    "        if opt.model_depth == 50:\n",
    "            model = resnext.resnet50(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                cardinality=opt.resnext_cardinality,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 101:\n",
    "            model = resnext.resnet101(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                cardinality=opt.resnext_cardinality,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 152:\n",
    "            model = resnext.resnet152(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                cardinality=opt.resnext_cardinality,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "    elif opt.model == 'preresnet':\n",
    "        assert opt.model_depth in [18, 34, 50, 101, 152, 200]\n",
    "\n",
    "        #from models.pre_act_resnet import get_fine_tuning_parameters\n",
    "\n",
    "        if opt.model_depth == 18:\n",
    "            model = pre_act_resnet.resnet18(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 34:\n",
    "            model = pre_act_resnet.resnet34(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 50:\n",
    "            model = pre_act_resnet.resnet50(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 101:\n",
    "            model = pre_act_resnet.resnet101(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 152:\n",
    "            model = pre_act_resnet.resnet152(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 200:\n",
    "            model = pre_act_resnet.resnet200(\n",
    "                num_classes=opt.n_classes,\n",
    "                shortcut_type=opt.resnet_shortcut,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "    elif opt.model == 'densenet':\n",
    "        assert opt.model_depth in [121, 169, 201, 264]\n",
    "\n",
    "        #from models.densenet import get_fine_tuning_parameters\n",
    "\n",
    "        if opt.model_depth == 121:\n",
    "            model = densenet.densenet121(\n",
    "                num_classes=opt.n_classes,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 169:\n",
    "            model = densenet.densenet169(\n",
    "                num_classes=opt.n_classes,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 201:\n",
    "            model = densenet.densenet201(\n",
    "                num_classes=opt.n_classes,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "        elif opt.model_depth == 264:\n",
    "            model = densenet.densenet264(\n",
    "                num_classes=opt.n_classes,\n",
    "                sample_size=opt.sample_size,\n",
    "                sample_duration=opt.sample_duration)\n",
    "\n",
    "    if not opt.no_cuda:\n",
    "        model = model.cuda()\n",
    "        model = nn.DataParallel(model, device_ids=None)\n",
    "\n",
    "        if opt.pretrain_path:\n",
    "            print('loading pretrained model {}'.format(opt.pretrain_path))\n",
    "            pretrain = torch.load(opt.pretrain_path)\n",
    "            assert opt.arch == pretrain['arch']\n",
    "\n",
    "            model.load_state_dict(pretrain['state_dict'])\n",
    "\n",
    "            if opt.model == 'densenet':\n",
    "                model.module.classifier = nn.Linear(\n",
    "                    model.module.classifier.in_features, opt.n_finetune_classes)\n",
    "                model.module.classifier = model.module.classifier.cuda()\n",
    "            else:\n",
    "                model.module.fc = nn.Linear(model.module.fc.in_features,\n",
    "                                            opt.n_finetune_classes)\n",
    "                model.module.fc = model.module.fc.cuda()\n",
    "\n",
    "            parameters = get_fine_tuning_parameters(model, opt.ft_begin_index)\n",
    "            return model, parameters\n",
    "    else:\n",
    "        if opt.pretrain_path:\n",
    "            print('loading pretrained model {}'.format(opt.pretrain_path))\n",
    "            pretrain = torch.load(opt.pretrain_path)\n",
    "            assert opt.arch == pretrain['arch']\n",
    "\n",
    "            model.load_state_dict(pretrain['state_dict'])\n",
    "\n",
    "            if opt.model == 'densenet':\n",
    "                model.classifier = nn.Linear(\n",
    "                    model.classifier.in_features, opt.n_finetune_classes)\n",
    "            else:\n",
    "                model.fc = nn.Linear(model.fc.in_features,\n",
    "                                            opt.n_finetune_classes)\n",
    "\n",
    "            parameters = get_fine_tuning_parameters(model, opt.ft_begin_index)\n",
    "            return model, parameters\n",
    "\n",
    "    return model, model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean(norm_value=255, dataset='activitynet'):\n",
    "    assert dataset in ['activitynet', 'kinetics']\n",
    "\n",
    "    if dataset == 'activitynet':\n",
    "        return [\n",
    "            114.7748 / norm_value, 107.7354 / norm_value, 99.4750 / norm_value\n",
    "        ]\n",
    "    elif dataset == 'kinetics':\n",
    "        # Kinetics (10 videos for each class)\n",
    "        return [\n",
    "            110.63666788 / norm_value, 103.16065604 / norm_value,\n",
    "            96.29023126 / norm_value\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_std(norm_value=255):\n",
    "    # Kinetics (10 videos for each class)\n",
    "    return [\n",
    "        38.7568578 / norm_value, 37.88248729 / norm_value,\n",
    "        40.02898126 / norm_value\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m_train\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from utils import AverageMeter, calculate_accuracy\n",
    "\n",
    "\n",
    "def train_epoch(epoch, data_loader, model, criterion, optimizer, opt,\n",
    "                epoch_logger, batch_logger):\n",
    "    print('train at epoch {}'.format(epoch))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"len: \",len(data_loader))\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        \n",
    "        print(\"debug: Train: targets : \",targets)\n",
    "\n",
    "\n",
    "        data_time.update(time.time() - end_time)\n",
    "\n",
    "        if not opt.no_cuda:\n",
    "            targets = targets.cuda(async=True)\n",
    "        inputs = Variable(inputs)\n",
    "        targets = Variable(targets)\n",
    "        print(\"debug: Train:inputTensor.size : \",inputs.size())\n",
    "        outputs = model(inputs)\n",
    "        print(\"debug: Train: outputTensor.size : \",outputs.size())\n",
    "\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        acc = calculate_accuracy(outputs, targets)\n",
    "\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        batch_logger.log({\n",
    "            'epoch': epoch,\n",
    "            'batch': i + 1,\n",
    "            'iter': (epoch - 1) * len(data_loader) + (i + 1),\n",
    "            'loss': losses.val,\n",
    "            'acc': accuracies.val,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                  epoch,\n",
    "                  i + 1,\n",
    "                  len(data_loader),\n",
    "                  batch_time=batch_time,\n",
    "                  data_time=data_time,\n",
    "                  loss=losses,\n",
    "                  acc=accuracies))\n",
    "\n",
    "    epoch_logger.log({\n",
    "        'epoch': epoch,\n",
    "        'loss': losses.avg,\n",
    "        'acc': accuracies.avg,\n",
    "        'lr': optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "    if epoch % opt.checkpoint == 0:\n",
    "        save_file_path = os.path.join(opt.result_path,\n",
    "                                      'save_{}.pth'.format(epoch))\n",
    "        states = {\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': opt.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(states, save_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, 'datasets/')\n",
    "\n",
    "# from kinetics import Kinetics\n",
    "# from activitynet import ActivityNet\n",
    "# from ucf101 import UCF101\n",
    "# from hmdb51 import HMDB51\n",
    "\n",
    "\n",
    "def get_training_set(opt, spatial_transform, temporal_transform,\n",
    "                     target_transform):\n",
    "    assert opt.dataset in ['kinetics', 'activitynet', 'ucf101', 'hmdb51']\n",
    "\n",
    "    if opt.dataset == 'kinetics':\n",
    "        training_data = Kinetics(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'training',\n",
    "            spatial_transform=spatial_transform,\n",
    "            temporal_transform=temporal_transform,\n",
    "            target_transform=target_transform)\n",
    "    elif opt.dataset == 'activitynet':\n",
    "        training_data = ActivityNet(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'training',\n",
    "            False,\n",
    "            spatial_transform=spatial_transform,\n",
    "            temporal_transform=temporal_transform,\n",
    "            target_transform=target_transform)\n",
    "    elif opt.dataset == 'ucf101':\n",
    "        training_data = UCF101(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'training',\n",
    "            spatial_transform=spatial_transform,\n",
    "            temporal_transform=temporal_transform,\n",
    "            target_transform=target_transform)\n",
    "    elif opt.dataset == 'hmdb51':\n",
    "        training_data = HMDB51(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'training',\n",
    "            spatial_transform=spatial_transform,\n",
    "            temporal_transform=temporal_transform,\n",
    "            target_transform=target_transform)\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "def get_validation_set(opt, spatial_transform, temporal_transform,\n",
    "                       target_transform):\n",
    "    assert opt.dataset in ['kinetics', 'activitynet', 'ucf101', 'hmdb51']\n",
    "\n",
    "    if opt.dataset == 'kinetics':\n",
    "        validation_data = Kinetics(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'validation',\n",
    "            opt.n_val_samples,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "    elif opt.dataset == 'activitynet':\n",
    "        validation_data = ActivityNet(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'validation',\n",
    "            False,\n",
    "            opt.n_val_samples,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "    elif opt.dataset == 'ucf101':\n",
    "        validation_data = UCF101(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'validation',\n",
    "            opt.n_val_samples,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "    elif opt.dataset == 'hmdb51':\n",
    "        validation_data = HMDB51(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            'validation',\n",
    "            opt.n_val_samples,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "    return validation_data\n",
    "\n",
    "\n",
    "def get_test_set(opt, spatial_transform, temporal_transform, target_transform):\n",
    "    assert opt.dataset in ['kinetics', 'activitynet', 'ucf101', 'hmdb51']\n",
    "    assert opt.test_subset in ['val', 'test']\n",
    "\n",
    "    if opt.test_subset == 'val':\n",
    "        subset = 'validation'\n",
    "    elif opt.test_subset == 'test':\n",
    "        subset = 'testing'\n",
    "    if opt.dataset == 'kinetics':\n",
    "        test_data = Kinetics(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            subset,\n",
    "            0,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration,\n",
    "            sample_stride=opt.sample_stride)\n",
    "    elif opt.dataset == 'activitynet':\n",
    "        test_data = ActivityNet(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            subset,\n",
    "            True,\n",
    "            0,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "    elif opt.dataset == 'ucf101':\n",
    "        test_data = UCF101(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            subset,\n",
    "            0,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "    elif opt.dataset == 'hmdb51':\n",
    "        test_data = HMDB51(\n",
    "            opt.video_path,\n",
    "            opt.annotation_path,\n",
    "            subset,\n",
    "            0,\n",
    "            spatial_transform,\n",
    "            temporal_transform,\n",
    "            target_transform,\n",
    "            sample_duration=opt.sample_duration)\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from utils import AverageMeter, calculate_accuracy\n",
    "\n",
    "\n",
    "def val_epoch(epoch, data_loader, model, criterion, opt, logger):\n",
    "    print('validation at epoch {}'.format(epoch))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracies = AverageMeter()\n",
    "\n",
    "    end_time = time.time()\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        data_time.update(time.time() - end_time)\n",
    "\n",
    "        if not opt.no_cuda:\n",
    "            targets = targets.cuda(async=True)\n",
    "        inputs = Variable(inputs, volatile=True)\n",
    "        targets = Variable(targets, volatile=True)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        acc = calculate_accuracy(outputs, targets)\n",
    "\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "              'Acc {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
    "                  epoch,\n",
    "                  i + 1,\n",
    "                  len(data_loader),\n",
    "                  batch_time=batch_time,\n",
    "                  data_time=data_time,\n",
    "                  loss=losses,\n",
    "                  acc=accuracies))\n",
    "\n",
    "    logger.log({'epoch': epoch, 'loss': losses.avg, 'acc': accuracies.avg})\n",
    "\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m_test\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from utils import AverageMeter\n",
    "\n",
    "\n",
    "def calculate_video_results(output_buffer, video_id, test_results, class_names):\n",
    "    video_outputs = torch.stack(output_buffer)\n",
    "    average_scores = torch.mean(video_outputs, dim=0)\n",
    "    sorted_scores, locs = torch.topk(average_scores, k=10)\n",
    "\n",
    "    video_results = []\n",
    "    for i in range(sorted_scores.size(0)):\n",
    "        video_results.append({\n",
    "            'label': class_names[locs[i]],\n",
    "            'score': sorted_scores[i]\n",
    "        })\n",
    "\n",
    "    test_results['results'][video_id] = video_results\n",
    "\n",
    "\n",
    "def test(data_loader, model, opt, class_names):\n",
    "    print('test')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    end_time = time.time()\n",
    "    output_buffer = []\n",
    "    previous_video_id = ''\n",
    "    test_results = {'results': {}}\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        print(\"debug: Test:target: \",targets)\n",
    "        data_time.update(time.time() - end_time)\n",
    "\n",
    "        inputs = Variable(inputs, volatile=True)\n",
    "        print(\"debug: Test:inputTensor.size : \",inputs.size())\n",
    "\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        print(\"debug: Test: outputTensor.size : \",outputs.size())\n",
    "        \n",
    "\n",
    "        if not opt.no_softmax_in_test:\n",
    "            outputs = F.softmax(outputs)\n",
    "        \n",
    "        for j in range(outputs.size(0)):\n",
    "            if not (i == 0 and j == 0) and targets[j] != previous_video_id:\n",
    "                calculate_video_results(output_buffer, previous_video_id,\n",
    "                                        test_results, class_names)\n",
    "                output_buffer = []\n",
    "            output_buffer.append(outputs[j].data.cpu())\n",
    "            previous_video_id = targets[j]\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            with open(\n",
    "                    os.path.join(opt.result_path, '{}.json'.format(\n",
    "                        opt.test_subset)), 'w') as f:\n",
    "                json.dump(test_results, f)\n",
    "\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print('[{}/{}]\\t'\n",
    "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "              'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'.format(\n",
    "                  i + 1,\n",
    "                  len(data_loader),\n",
    "                  batch_time=batch_time,\n",
    "                  data_time=data_time))\n",
    "    with open(\n",
    "            os.path.join(opt.result_path, '{}.json'.format(opt.test_subset)),\n",
    "            'w') as f:\n",
    "        json.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n",
      "<class 'argparse.ArgumentParser'>\n",
      "almost done\n",
      "done\n",
      "Namespace(annotation_path='data/activitynet.json', arch='resnet-10', batch_size=1, begin_epoch=1, checkpoint=10, crop_position_in_test='c', dampening=0.9, dataset='activitynet', ft_begin_index=0, initial_scale=1.0, learning_rate=0.1, lr_patience=10, manual_seed=1, mean=[114.7748, 107.7354, 99.475], mean_dataset='activitynet', model='resnet', model_depth=10, momentum=0.9, n_classes=200, n_epochs=3, n_finetune_classes=400, n_scales=5, n_threads=4, n_val_samples=3, nesterov=False, no_cuda=True, no_hflip=False, no_mean_norm=False, no_softmax_in_test=False, no_train=False, no_val=True, norm_value=1, optimizer='sgd', pretrain_path='', resnet_shortcut='B', resnext_cardinality=32, result_path='data/results', resume_path='', root_path='data', sample_duration=16, sample_size=112, scale_in_test=1.0, scale_step=0.84089641525, scales=[1.0, 0.84089641525, 0.7071067811803005, 0.5946035574934808, 0.4999999999911653], std=[38.7568578, 37.88248729, 40.02898126], std_norm=False, test=False, test_subset='val', train_crop='corner', video_path='data/jpg_videos', weight_decay=0.001, wide_resnet_k=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:145: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool3d(kernel_size=(1, 4, 4), stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=200, bias=True)\n",
      ")\n",
      "dataset loading [0/4819]\n",
      "Caption found for  v_---9CpRcKoU\n",
      "Caption found for  v_-AjZCBMb4qU\n",
      "dataset loading [1000/4819]\n",
      "dataset loading [2000/4819]\n",
      "dataset loading [3000/4819]\n",
      "dataset loading [4000/4819]\n",
      "training data length:  5\n",
      "run\n",
      "train at epoch 1\n",
      "len:  5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Traceback (most recent call last):\\n  File \"/Users/mbajaj/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\\n    samples = collate_fn([dataset[i] for i in batch_indices])\\n  File \"/Users/mbajaj/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\\n    samples = collate_fn([dataset[i] for i in batch_indices])\\n  File \"<ipython-input-26-6b10a0ce5cae>\", line 380, in __getitem__\\n    target = self.target_transform(target)\\n  File \"/Users/mbajaj/git_cv/3D-ResNets-PyTorch/target_transforms.py\", line 20, in __call__\\n    return target[\\'label\\']\\nKeyError: \\'label\\'\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8b194306173c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             train_epoch(i, train_loader, model, criterion, optimizer, opt,\n\u001b[0;32m--> 141\u001b[0;31m                         train_logger, train_batch_logger)\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             validation_loss = val_epoch(i, val_loader, model, criterion, opt,\n",
      "\u001b[0;32m<ipython-input-21-5cf4535c7c35>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, data_loader, model, criterion, optimizer, opt, epoch_logger, batch_logger)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"debug: Train: targets : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mbajaj/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mbajaj/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Traceback (most recent call last):\\n  File \"/Users/mbajaj/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\\n    samples = collate_fn([dataset[i] for i in batch_indices])\\n  File \"/Users/mbajaj/anaconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\\n    samples = collate_fn([dataset[i] for i in batch_indices])\\n  File \"<ipython-input-26-6b10a0ce5cae>\", line 380, in __getitem__\\n    target = self.target_transform(target)\\n  File \"/Users/mbajaj/git_cv/3D-ResNets-PyTorch/target_transforms.py\", line 20, in __call__\\n    return target[\\'label\\']\\nKeyError: \\'label\\'\\n'"
     ]
    }
   ],
   "source": [
    "#m_main\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# #from opts import parse_opts\n",
    "# #from model import generate_model\n",
    "# #from mean import get_mean, get_std\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, ToTensor)\n",
    "from temporal_transforms import LoopPadding, TemporalRandomCrop\n",
    "from target_transforms import ClassLabel, VideoID\n",
    "from target_transforms import Compose as TargetCompose\n",
    "# # from dataset import get_training_set, get_validation_set, get_test_set\n",
    "from utils import Logger\n",
    "# # from train import train_epoch\n",
    "# # from validation import val_epoch\n",
    "# #import test\n",
    "print(__name__)\n",
    "if __name__ == '__main__':\n",
    "    opt = parse_opts()\n",
    "    #print(opt)\n",
    "    if opt.root_path != '':\n",
    "        opt.video_path = os.path.join(opt.root_path, opt.video_path)\n",
    "        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\n",
    "        opt.result_path = os.path.join(opt.root_path, opt.result_path)\n",
    "        if opt.resume_path:\n",
    "            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\n",
    "        if opt.pretrain_path:\n",
    "            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\n",
    "    opt.scales = [opt.initial_scale]\n",
    "    for i in range(1, opt.n_scales):\n",
    "        opt.scales.append(opt.scales[-1] * opt.scale_step)\n",
    "    opt.arch = '{}-{}'.format(opt.model, opt.model_depth)\n",
    "    opt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)\n",
    "    opt.std = get_std(opt.norm_value)\n",
    "    print(opt)\n",
    "    with open(os.path.join(opt.result_path, 'opts.json'), 'w') as opt_file:\n",
    "        json.dump(vars(opt), opt_file)\n",
    "\n",
    "    torch.manual_seed(opt.manual_seed)\n",
    "\n",
    "    model, parameters = generate_model(opt)\n",
    "    print(model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if not opt.no_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    if opt.no_mean_norm and not opt.std_norm:\n",
    "        norm_method = Normalize([0, 0, 0], [1, 1, 1])\n",
    "    elif not opt.std_norm:\n",
    "        norm_method = Normalize(opt.mean, [1, 1, 1])\n",
    "    else:\n",
    "        norm_method = Normalize(opt.mean, opt.std)\n",
    "\n",
    "    if not opt.no_train:\n",
    "        assert opt.train_crop in ['random', 'corner', 'center']\n",
    "        if opt.train_crop == 'random':\n",
    "            crop_method = MultiScaleRandomCrop(opt.scales, opt.sample_size)\n",
    "        elif opt.train_crop == 'corner':\n",
    "            crop_method = MultiScaleCornerCrop(opt.scales, opt.sample_size)\n",
    "        elif opt.train_crop == 'center':\n",
    "            crop_method = MultiScaleCornerCrop(\n",
    "                opt.scales, opt.sample_size, crop_positions=['c'])\n",
    "        spatial_transform = Compose([\n",
    "            crop_method,\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(opt.norm_value), norm_method\n",
    "        ])\n",
    "        temporal_transform = TemporalRandomCrop(opt.sample_duration)\n",
    "        target_transform = ClassLabel()\n",
    "        training_data = get_training_set(opt, spatial_transform,\n",
    "                                         temporal_transform, target_transform)\n",
    "        print(\"training data length: \", len(training_data))\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            training_data,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=opt.n_threads,\n",
    "            pin_memory=True)\n",
    "        train_logger = Logger(\n",
    "            os.path.join(opt.result_path, 'train.log'),\n",
    "            ['epoch', 'loss', 'acc', 'lr'])\n",
    "        train_batch_logger = Logger(\n",
    "            os.path.join(opt.result_path, 'train_batch.log'),\n",
    "            ['epoch', 'batch', 'iter', 'loss', 'acc', 'lr'])\n",
    "        \n",
    "        if opt.nesterov:\n",
    "            dampening = 0\n",
    "        else:\n",
    "            dampening = opt.dampening\n",
    "        optimizer = optim.SGD(\n",
    "            parameters,\n",
    "            lr=opt.learning_rate,\n",
    "            momentum=opt.momentum,\n",
    "            dampening=dampening,\n",
    "            weight_decay=opt.weight_decay,\n",
    "            nesterov=opt.nesterov)\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, 'min', patience=opt.lr_patience)\n",
    "    if not opt.no_val:\n",
    "        spatial_transform = Compose([\n",
    "            Scale(opt.sample_size),\n",
    "            CenterCrop(opt.sample_size),\n",
    "            ToTensor(opt.norm_value), norm_method\n",
    "        ])\n",
    "        temporal_transform = LoopPadding(opt.sample_duration)\n",
    "        target_transform = ClassLabel()\n",
    "        validation_data = get_validation_set(\n",
    "            opt, spatial_transform, temporal_transform, target_transform)\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            validation_data,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=opt.n_threads,\n",
    "            pin_memory=True)\n",
    "        val_logger = Logger(\n",
    "            os.path.join(opt.result_path, 'val.log'), ['epoch', 'loss', 'acc'])\n",
    "\n",
    "    if opt.resume_path:\n",
    "        print('loading checkpoint {}'.format(opt.resume_path))\n",
    "        checkpoint = torch.load(opt.resume_path)\n",
    "        assert opt.arch == checkpoint['arch']\n",
    "\n",
    "        opt.begin_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        if not opt.no_train:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    print('run')\n",
    "    for i in range(opt.begin_epoch, opt.n_epochs + 1):\n",
    "        if not opt.no_train:\n",
    "            train_epoch(i, train_loader, model, criterion, optimizer, opt,\n",
    "                        train_logger, train_batch_logger)\n",
    "        if not opt.no_val:\n",
    "            validation_loss = val_epoch(i, val_loader, model, criterion, opt,\n",
    "                                        val_logger)\n",
    "\n",
    "        if not opt.no_train and not opt.no_val:\n",
    "            scheduler.step(validation_loss)\n",
    "\n",
    "    if opt.test:\n",
    "        spatial_transform = Compose([\n",
    "            Scale(int(opt.sample_size / opt.scale_in_test)),\n",
    "            CornerCrop(opt.sample_size, opt.crop_position_in_test),\n",
    "            ToTensor(opt.norm_value), norm_method\n",
    "        ])\n",
    "        temporal_transform = LoopPadding(opt.sample_duration)\n",
    "        target_transform = VideoID()\n",
    "\n",
    "        test_data = get_test_set(opt, spatial_transform, temporal_transform,\n",
    "                                 target_transform)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=opt.n_threads,\n",
    "            pin_memory=True)\n",
    "        test(test_loader, model, opt, test_data.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (<ipython-input-22-876fc7f1f69b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-876fc7f1f69b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    *l\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "*l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('--1DO2V4K74',)\n",
      "1 ('--1DO2V4K74',)\n",
      "2 ('--1DO2V4K74',)\n",
      "3 ('--1DO2V4K74',)\n",
      "4 ('--1DO2V4K74',)\n",
      "5 ('--1DO2V4K74',)\n",
      "6 ('--1DO2V4K74',)\n",
      "7 ('--1DO2V4K74',)\n",
      "8 ('--1DO2V4K74',)\n",
      "9 ('--1DO2V4K74',)\n",
      "10 ('--1DO2V4K74',)\n",
      "11 ('--1DO2V4K74',)\n",
      "12 ('--1DO2V4K74',)\n",
      "13 ('--1DO2V4K74',)\n",
      "14 ('--1DO2V4K74',)\n",
      "15 ('--1DO2V4K74',)\n",
      "16 ('--1DO2V4K74',)\n",
      "17 ('--1DO2V4K74',)\n",
      "18 ('--1DO2V4K74',)\n",
      "19 ('--1DO2V4K74',)\n",
      "20 ('--1DO2V4K74',)\n",
      "21 ('--1DO2V4K74',)\n",
      "22 ('--1DO2V4K74',)\n",
      "23 ('--1DO2V4K74',)\n",
      "24 ('--1DO2V4K74',)\n",
      "25 ('--1DO2V4K74',)\n",
      "26 ('--1DO2V4K74',)\n",
      "27 ('--1DO2V4K74',)\n",
      "28 ('--1DO2V4K74',)\n",
      "29 ('--1DO2V4K74',)\n",
      "30 ('--1DO2V4K74',)\n",
      "31 ('--1DO2V4K74',)\n",
      "32 ('--1DO2V4K74',)\n",
      "33 ('--1DO2V4K74',)\n",
      "34 ('--1DO2V4K74',)\n",
      "35 ('--1DO2V4K74',)\n",
      "36 ('--1DO2V4K74',)\n",
      "37 ('--1DO2V4K74',)\n",
      "38 ('--1DO2V4K74',)\n",
      "39 ('--1DO2V4K74',)\n",
      "40 ('--1DO2V4K74',)\n",
      "41 ('--1DO2V4K74',)\n",
      "42 ('--1DO2V4K74',)\n",
      "43 ('--1DO2V4K74',)\n",
      "44 ('--1DO2V4K74',)\n",
      "45 ('--1DO2V4K74',)\n",
      "46 ('--1DO2V4K74',)\n",
      "47 ('--1DO2V4K74',)\n",
      "48 ('--1DO2V4K74',)\n",
      "49 ('--1DO2V4K74',)\n",
      "50 ('--1DO2V4K74',)\n",
      "51 ('--1DO2V4K74',)\n",
      "52 ('--1DO2V4K74',)\n",
      "53 ('--1DO2V4K74',)\n",
      "54 ('--1DO2V4K74',)\n",
      "55 ('--1DO2V4K74',)\n",
      "56 ('--1DO2V4K74',)\n",
      "57 ('--1DO2V4K74',)\n",
      "58 ('--1DO2V4K74',)\n",
      "59 ('--1DO2V4K74',)\n",
      "60 ('--1DO2V4K74',)\n",
      "61 ('--1DO2V4K74',)\n",
      "62 ('--1DO2V4K74',)\n",
      "63 ('--1DO2V4K74',)\n",
      "64 ('--1DO2V4K74',)\n",
      "65 ('--1DO2V4K74',)\n",
      "66 ('--1DO2V4K74',)\n",
      "67 ('--1DO2V4K74',)\n",
      "68 ('--1DO2V4K74',)\n",
      "69 ('--1DO2V4K74',)\n",
      "70 ('--1DO2V4K74',)\n",
      "71 ('--1DO2V4K74',)\n",
      "72 ('--1DO2V4K74',)\n",
      "73 ('--1DO2V4K74',)\n",
      "74 ('--1DO2V4K74',)\n",
      "75 ('--1DO2V4K74',)\n",
      "76 ('--1DO2V4K74',)\n",
      "77 ('--1DO2V4K74',)\n",
      "78 ('--1DO2V4K74',)\n",
      "79 ('--1DO2V4K74',)\n",
      "80 ('--1DO2V4K74',)\n",
      "81 ('--1DO2V4K74',)\n",
      "82 ('--1DO2V4K74',)\n",
      "83 ('--1DO2V4K74',)\n",
      "84 ('--1DO2V4K74',)\n",
      "85 ('--1DO2V4K74',)\n",
      "86 ('--1DO2V4K74',)\n",
      "87 ('--1DO2V4K74',)\n",
      "88 ('--1DO2V4K74',)\n",
      "89 ('--1DO2V4K74',)\n",
      "90 ('--1DO2V4K74',)\n",
      "91 ('--1DO2V4K74',)\n",
      "92 ('--1DO2V4K74',)\n",
      "93 ('--1DO2V4K74',)\n",
      "94 ('--1DO2V4K74',)\n",
      "95 ('--1DO2V4K74',)\n",
      "96 ('--1DO2V4K74',)\n",
      "97 ('--1DO2V4K74',)\n",
      "98 ('--1DO2V4K74',)\n",
      "99 ('--1DO2V4K74',)\n",
      "100 ('--1DO2V4K74',)\n",
      "101 ('--1DO2V4K74',)\n",
      "102 ('--1DO2V4K74',)\n",
      "103 ('--1DO2V4K74',)\n",
      "104 ('--1DO2V4K74',)\n",
      "105 ('--1DO2V4K74',)\n",
      "106 ('--1DO2V4K74',)\n",
      "107 ('--1DO2V4K74',)\n",
      "108 ('--1DO2V4K74',)\n",
      "109 ('--1DO2V4K74',)\n",
      "110 ('--1DO2V4K74',)\n",
      "111 ('--1DO2V4K74',)\n",
      "112 ('--1DO2V4K74',)\n",
      "113 ('--1DO2V4K74',)\n",
      "114 ('--1DO2V4K74',)\n",
      "115 ('--1DO2V4K74',)\n",
      "116 ('--1DO2V4K74',)\n",
      "117 ('--1DO2V4K74',)\n",
      "118 ('--1DO2V4K74',)\n",
      "119 ('--1DO2V4K74',)\n",
      "120 ('--1DO2V4K74',)\n",
      "121 ('--1DO2V4K74',)\n",
      "122 ('--1DO2V4K74',)\n",
      "123 ('--1DO2V4K74',)\n",
      "124 ('--1DO2V4K74',)\n",
      "125 ('--1DO2V4K74',)\n",
      "126 ('--1DO2V4K74',)\n",
      "127 ('--1DO2V4K74',)\n",
      "128 ('--1DO2V4K74',)\n",
      "129 ('--1DO2V4K74',)\n",
      "130 ('--1DO2V4K74',)\n",
      "131 ('--1DO2V4K74',)\n",
      "132 ('--1DO2V4K74',)\n",
      "133 ('--1DO2V4K74',)\n",
      "134 ('--1DO2V4K74',)\n",
      "135 ('--1DO2V4K74',)\n",
      "136 ('--1DO2V4K74',)\n",
      "137 ('--1DO2V4K74',)\n",
      "138 ('--1DO2V4K74',)\n",
      "139 ('--1DO2V4K74',)\n",
      "140 ('--1DO2V4K74',)\n",
      "141 ('--1DO2V4K74',)\n",
      "142 ('--1DO2V4K74',)\n",
      "143 ('--1DO2V4K74',)\n",
      "144 ('--1DO2V4K74',)\n",
      "145 ('--1DO2V4K74',)\n",
      "146 ('--1DO2V4K74',)\n",
      "147 ('--1DO2V4K74',)\n",
      "148 ('--1DO2V4K74',)\n",
      "149 ('--1DO2V4K74',)\n",
      "150 ('--1DO2V4K74',)\n",
      "151 ('--1DO2V4K74',)\n",
      "152 ('--1DO2V4K74',)\n",
      "153 ('--1DO2V4K74',)\n",
      "154 ('--1DO2V4K74',)\n",
      "155 ('--1DO2V4K74',)\n",
      "156 ('--1DO2V4K74',)\n",
      "157 ('--1DO2V4K74',)\n",
      "158 ('--1DO2V4K74',)\n",
      "159 ('--1DO2V4K74',)\n",
      "160 ('--1DO2V4K74',)\n",
      "161 ('--1DO2V4K74',)\n",
      "162 ('--1DO2V4K74',)\n",
      "163 ('--1DO2V4K74',)\n",
      "164 ('--1DO2V4K74',)\n",
      "165 ('--1DO2V4K74',)\n",
      "166 ('--1DO2V4K74',)\n",
      "167 ('--1DO2V4K74',)\n",
      "168 ('--1DO2V4K74',)\n",
      "169 ('--1DO2V4K74',)\n",
      "170 ('--1DO2V4K74',)\n",
      "171 ('--1DO2V4K74',)\n",
      "172 ('--1DO2V4K74',)\n",
      "173 ('--1DO2V4K74',)\n",
      "174 ('--1DO2V4K74',)\n",
      "175 ('--1DO2V4K74',)\n",
      "176 ('--1DO2V4K74',)\n",
      "177 ('--1DO2V4K74',)\n",
      "178 ('--1DO2V4K74',)\n",
      "179 ('--1DO2V4K74',)\n",
      "180 ('--1DO2V4K74',)\n",
      "181 ('--1DO2V4K74',)\n",
      "182 ('--1DO2V4K74',)\n",
      "183 ('--1DO2V4K74',)\n",
      "184 ('--1DO2V4K74',)\n",
      "185 ('--1DO2V4K74',)\n",
      "186 ('--1DO2V4K74',)\n",
      "187 ('--1DO2V4K74',)\n",
      "188 ('--1DO2V4K74',)\n",
      "189 ('--1DO2V4K74',)\n",
      "190 ('--1DO2V4K74',)\n",
      "191 ('--1DO2V4K74',)\n",
      "192 ('--1DO2V4K74',)\n",
      "193 ('--1DO2V4K74',)\n",
      "194 ('--1DO2V4K74',)\n",
      "195 ('--1DO2V4K74',)\n",
      "196 ('--1DO2V4K74',)\n",
      "197 ('--1DO2V4K74',)\n",
      "198 ('--1DO2V4K74',)\n",
      "199 ('--1DO2V4K74',)\n",
      "200 ('--1DO2V4K74',)\n",
      "201 ('--1DO2V4K74',)\n",
      "202 ('--1DO2V4K74',)\n",
      "203 ('--1DO2V4K74',)\n",
      "204 ('--1DO2V4K74',)\n",
      "205 ('--1DO2V4K74',)\n",
      "206 ('--1DO2V4K74',)\n",
      "207 ('--1DO2V4K74',)\n",
      "208 ('--1DO2V4K74',)\n",
      "209 ('--1DO2V4K74',)\n",
      "210 ('--1DO2V4K74',)\n",
      "211 ('--1DO2V4K74',)\n",
      "212 ('--1DO2V4K74',)\n",
      "213 ('--1DO2V4K74',)\n",
      "214 ('--1DO2V4K74',)\n",
      "215 ('--1DO2V4K74',)\n",
      "216 ('--1DO2V4K74',)\n",
      "217 ('--1DO2V4K74',)\n",
      "218 ('--1DO2V4K74',)\n",
      "219 ('--1DO2V4K74',)\n",
      "220 ('--1DO2V4K74',)\n",
      "221 ('--1DO2V4K74',)\n",
      "222 ('--1DO2V4K74',)\n",
      "223 ('--1DO2V4K74',)\n",
      "224 ('--1DO2V4K74',)\n",
      "225 ('--1DO2V4K74',)\n",
      "226 ('--1DO2V4K74',)\n",
      "227 ('--1DO2V4K74',)\n",
      "228 ('--1DO2V4K74',)\n",
      "229 ('--1DO2V4K74',)\n",
      "230 ('--1DO2V4K74',)\n",
      "231 ('--1DO2V4K74',)\n",
      "232 ('--1DO2V4K74',)\n",
      "233 ('--1DO2V4K74',)\n",
      "234 ('--1DO2V4K74',)\n",
      "235 ('--1DO2V4K74',)\n",
      "236 ('--1DO2V4K74',)\n",
      "237 ('--1DO2V4K74',)\n",
      "238 ('--1DO2V4K74',)\n",
      "239 ('--1DO2V4K74',)\n",
      "240 ('--1DO2V4K74',)\n",
      "241 ('--1DO2V4K74',)\n",
      "242 ('--1DO2V4K74',)\n",
      "243 ('--1DO2V4K74',)\n",
      "244 ('--1DO2V4K74',)\n",
      "245 ('--1DO2V4K74',)\n",
      "246 ('--1DO2V4K74',)\n",
      "247 ('--1DO2V4K74',)\n",
      "248 ('--1DO2V4K74',)\n",
      "249 ('--1DO2V4K74',)\n",
      "250 ('--1DO2V4K74',)\n",
      "251 ('--1DO2V4K74',)\n",
      "252 ('--1DO2V4K74',)\n",
      "253 ('--1DO2V4K74',)\n",
      "254 ('--1DO2V4K74',)\n",
      "255 ('--1DO2V4K74',)\n",
      "256 ('--1DO2V4K74',)\n",
      "257 ('--1DO2V4K74',)\n",
      "258 ('--1DO2V4K74',)\n",
      "259 ('--1DO2V4K74',)\n",
      "260 ('--1DO2V4K74',)\n",
      "261 ('--1DO2V4K74',)\n",
      "262 ('--1DO2V4K74',)\n",
      "263 ('--1DO2V4K74',)\n",
      "264 ('--1DO2V4K74',)\n",
      "265 ('--1DO2V4K74',)\n",
      "266 ('--1DO2V4K74',)\n",
      "267 ('--1DO2V4K74',)\n",
      "268 ('--1DO2V4K74',)\n",
      "269 ('--1DO2V4K74',)\n",
      "270 ('--1DO2V4K74',)\n",
      "271 ('--1DO2V4K74',)\n",
      "272 ('--1DO2V4K74',)\n",
      "273 ('--1DO2V4K74',)\n",
      "274 ('--1DO2V4K74',)\n",
      "275 ('--1DO2V4K74',)\n",
      "276 ('--1DO2V4K74',)\n",
      "277 ('--1DO2V4K74',)\n",
      "278 ('--1DO2V4K74',)\n",
      "279 ('--1DO2V4K74',)\n",
      "280 ('--1DO2V4K74',)\n",
      "281 ('--1DO2V4K74',)\n",
      "282 ('--1DO2V4K74',)\n",
      "283 ('--1DO2V4K74',)\n",
      "284 ('--1DO2V4K74',)\n",
      "285 ('--1DO2V4K74',)\n",
      "286 ('--1DO2V4K74',)\n",
      "287 ('--1DO2V4K74',)\n",
      "288 ('--1DO2V4K74',)\n",
      "289 ('--1DO2V4K74',)\n",
      "290 ('--1DO2V4K74',)\n",
      "291 ('--1DO2V4K74',)\n",
      "292 ('--1DO2V4K74',)\n",
      "293 ('--1DO2V4K74',)\n",
      "294 ('--1DO2V4K74',)\n",
      "295 ('--1DO2V4K74',)\n",
      "296 ('--1DO2V4K74',)\n",
      "297 ('--1DO2V4K74',)\n",
      "298 ('--1DO2V4K74',)\n",
      "299 ('--1DO2V4K74',)\n",
      "300 ('--1DO2V4K74',)\n",
      "301 ('--1DO2V4K74',)\n",
      "302 ('--1DO2V4K74',)\n",
      "303 ('--1DO2V4K74',)\n",
      "304 ('--1DO2V4K74',)\n",
      "305 ('--1DO2V4K74',)\n",
      "306 ('--1DO2V4K74',)\n",
      "307 ('--1DO2V4K74',)\n",
      "308 ('--1DO2V4K74',)\n",
      "309 ('--1DO2V4K74',)\n",
      "310 ('--1DO2V4K74',)\n",
      "311 ('--1DO2V4K74',)\n",
      "312 ('--1DO2V4K74',)\n",
      "313 ('--1DO2V4K74',)\n",
      "314 ('--1DO2V4K74',)\n",
      "315 ('--1DO2V4K74',)\n",
      "316 ('--1DO2V4K74',)\n",
      "317 ('--1DO2V4K74',)\n",
      "318 ('--1DO2V4K74',)\n",
      "319 ('--1DO2V4K74',)\n",
      "320 ('--1DO2V4K74',)\n",
      "321 ('--1DO2V4K74',)\n",
      "322 ('--1DO2V4K74',)\n",
      "323 ('--1DO2V4K74',)\n",
      "324 ('--1DO2V4K74',)\n",
      "325 ('--1DO2V4K74',)\n",
      "326 ('--1DO2V4K74',)\n",
      "327 ('--1DO2V4K74',)\n",
      "328 ('--1DO2V4K74',)\n",
      "329 ('--1DO2V4K74',)\n",
      "330 ('--1DO2V4K74',)\n",
      "331 ('--1DO2V4K74',)\n",
      "332 ('--1DO2V4K74',)\n",
      "333 ('--1DO2V4K74',)\n",
      "334 ('--1DO2V4K74',)\n",
      "335 ('--1DO2V4K74',)\n",
      "336 ('--1DO2V4K74',)\n",
      "337 ('--1DO2V4K74',)\n",
      "338 ('--1DO2V4K74',)\n",
      "339 ('--1DO2V4K74',)\n",
      "340 ('--1DO2V4K74',)\n",
      "341 ('--1DO2V4K74',)\n",
      "342 ('--1DO2V4K74',)\n",
      "343 ('--1DO2V4K74',)\n",
      "344 ('--1DO2V4K74',)\n",
      "345 ('--1DO2V4K74',)\n",
      "346 ('--1DO2V4K74',)\n",
      "347 ('--1DO2V4K74',)\n",
      "348 ('--1DO2V4K74',)\n",
      "349 ('--1DO2V4K74',)\n",
      "350 ('--1DO2V4K74',)\n",
      "351 ('--1DO2V4K74',)\n",
      "352 ('--1DO2V4K74',)\n",
      "353 ('--1DO2V4K74',)\n",
      "354 ('--1DO2V4K74',)\n",
      "355 ('--1DO2V4K74',)\n",
      "356 ('--1DO2V4K74',)\n",
      "357 ('--1DO2V4K74',)\n",
      "358 ('--1DO2V4K74',)\n",
      "359 ('--1DO2V4K74',)\n",
      "360 ('--1DO2V4K74',)\n",
      "361 ('--1DO2V4K74',)\n",
      "362 ('--1DO2V4K74',)\n",
      "363 ('--1DO2V4K74',)\n",
      "364 ('--1DO2V4K74',)\n",
      "365 ('--1DO2V4K74',)\n",
      "366 ('--1DO2V4K74',)\n",
      "367 ('--1DO2V4K74',)\n",
      "368 ('--1DO2V4K74',)\n",
      "369 ('--1DO2V4K74',)\n",
      "370 ('--1DO2V4K74',)\n",
      "371 ('--1DO2V4K74',)\n",
      "372 ('--1DO2V4K74',)\n",
      "373 ('--1DO2V4K74',)\n",
      "374 ('--1DO2V4K74',)\n",
      "375 ('--1DO2V4K74',)\n",
      "376 ('--1DO2V4K74',)\n",
      "377 ('--1DO2V4K74',)\n",
      "378 ('--1DO2V4K74',)\n",
      "379 ('--1DO2V4K74',)\n",
      "380 ('--1DO2V4K74',)\n",
      "381 ('--1DO2V4K74',)\n",
      "382 ('--1DO2V4K74',)\n",
      "383 ('--1DO2V4K74',)\n",
      "384 ('--1DO2V4K74',)\n",
      "385 ('--1DO2V4K74',)\n",
      "386 ('--1DO2V4K74',)\n",
      "387 ('--1DO2V4K74',)\n",
      "388 ('--1DO2V4K74',)\n",
      "389 ('--1DO2V4K74',)\n",
      "390 ('--1DO2V4K74',)\n",
      "391 ('--1DO2V4K74',)\n",
      "392 ('--1DO2V4K74',)\n",
      "393 ('--1DO2V4K74',)\n",
      "394 ('--1DO2V4K74',)\n",
      "395 ('--1DO2V4K74',)\n",
      "2\n",
      "torch.Size([1, 3, 16, 112, 112])\n",
      "('--1DO2V4K74',)\n"
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(test_loader):\n",
    "    print(i,data[1])\n",
    "print(len(data))\n",
    "print(data[0].shape)\n",
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "{'video': 'data/jpg_videos/v_--1DO2V4K74', 'segment': [1, 6337], 'fps': 29.96, 'video_id': '--1DO2V4K74', 'frame_indices': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}\n"
     ]
    }
   ],
   "source": [
    "video_path = test_loader.dataset.data[0]['video']\n",
    "fps_file_path = os.path.join(video_path, 'fps')\n",
    "fps_file_path\n",
    "print(len(test_loader.dataset))\n",
    "print(test_loader.dataset.data[0])\n",
    "#print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. 10. 20. 30. 40. 50. 59. 69. 79. 89. 99.]\n"
     ]
    }
   ],
   "source": [
    "samples = np.round(np.linspace(0, 99, 11))\n",
    "#samples = np.linspace(0, 99, 11)\n",
    "\n",
    "\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = train_loader.dataset.data[0]['video']\n",
    "# fps_file_path = os.path.join(video_path, 'fps')\n",
    "# fps_file_path\n",
    "print(len(train_loader.dataset))\n",
    "print(train_loader.dataset.data[2])\n",
    "#print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
